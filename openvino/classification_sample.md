## OpenVINO Classification Sample

#### 1. OpenMP Thread Affinity Control (MKLDNN / MKL)
The Intel® OpenMP* runtime library has the ability to bind OpenMP threads to physical processing units.
The interface is controlled using the KMP_AFFINITY environment variable, and with compilers version 13.1.0 (Intel® Composer XE 2013 Update 2)
and newer, the KMP_PLACE_THREADS environment variable.

##### 1.1 There are 2 Considerations for OpenMP Threading and Affinity:
* First, determine the number of threads to utilize,
* Secondly, how to bind threads to specific processor cores.

##### 1.2 Threading
The Intel® Xeon Phi™ Coprocessor supports 4 thread contexts per core. So an initial consideration is how many application threads are optimal for this processor?
```
This will depend on your application.
In general, more threads help to hide latencies inherent in your application:
    ▫ while 1 thread is stalled waiting for memory,
      another 1-3 threads could schedule on the processor.
```

* **Why N-1 instead of N?**
```
OS overhead - 
The OS and MPSS threads do take processor cycles and it is inefficient to 
schedule worker threads on cores where the OS threads are contenting for cycles.
```

##### 1.3 Place Threads on Cores (Affinity)

##### what core are the OS threads using and how can I avoid scheduling threads on that core used by OS threads?

Here is the ONLINE doc for Thread Affinity Interface  and make sure you fully understand COMPACT, SCATTER, and BALANCED affinity types. 
```
please refer:
https://software.intel.com/en-us/node/522691
https://software.intel.com/en-us/node/522691#AFFINITY_TYPES
```

If you are very familiar with OpenMP thread affinity control and just want a quick advice of the processor mapping, for MIC try these two options for KMP_AFFINITY:
```
export MIC_ENV_PREFIX=PHI
export PHI_KMP_AFFINITY=granularity=fine,balanced
or
export PHI_KMP_AFFINITY=granularity=fine,compact
```

TIP: use the VERBOSE modifier on KMP_AFFINITY to get a detailed list of bindings. 
```
Example:
    export PHI_KMP_AFFINITY="verbose,granularity=fine,balanced"
```

##### If you do not set a value for KMP_AFFINITY, the OpenMP runtime is allowed to choose affinity for you.
##### Thus, the advice is that if your application can take advantage of a certain affinity setting then you should explicitly specify that setting. 

Consider avoiding conflicts with OS threads by avoiding the core running OS threads. Some sample settings to try:
* KMP_AFFINITY=granularity=fine,balanced
* KMP_AFFINITY=granularity=fine,compact 

###### reference 
https://software.intel.com/en-us/articles/openmp-thread-affinity-control


#### 2. OpenVINO Classification Sample

##### Read Image/Images from Specified Argument
```cpp
std::vector<std::string> imageNames;
parseInputFilesArguments(imageNames);

// Helper Functions - Parse Arguments and List Images in the Folder.

void parseInputFilesArguments(std::vector<std::string> &files) {
    std::vector<std::string> args = gflags::GetArgvs();
    bool readArguments = false;
    for (size_t i = 0; i < args.size(); i++) {
        if (args.at(i) == "-i" || args.at(i) == "--images") {
            readArguments = true;
            continue;
        }
        if (!readArguments) {
            continue;
        }
        if (args.at(i).c_str()[0] == '-') {
            break;
        }
        readInputFilesArguments(files, args.at(i));
    }
}

void readInputFilesArguments(std::vector<std::string> &files, const std::string& arg) {
    struct stat sb;
    if (stat(arg.c_str(), &sb) != 0) {
        slog::warn << "File " << arg << " cannot be opened!" << slog::endl;
        return;
    }
    if (S_ISDIR(sb.st_mode)) {
        DIR *dp;
        dp = opendir(arg.c_str());
        if (dp == nullptr) {
            slog::warn << "Directory " << arg << " cannot be opened!" << slog::endl;
            return;
        }

        struct dirent *ep;
        while (nullptr != (ep = readdir(dp))) {
            std::string fileName = ep->d_name;
            if (fileName == "." || fileName == "..") continue;
            files.push_back(arg + "/" + ep->d_name);
        }
        closedir(dp);
    } else {
        files.push_back(arg);
    }

    if (files.size() < 20) {
        slog::info << "Files were added: " << files.size() << slog::endl;
        for (std::string filePath : files) {
            slog::info << "    " << filePath << slog::endl;
        }
    } else {
        slog::info << "Files were added: " << files.size() << ". Too many to display each of them." << slog::endl;
    }
}
```

##### 2.1 Part I - Load Plugin for inference engine
```cpp
InferencePlugin plugin = PluginDispatcher({ FLAGS_pp, "../../../lib/intel64" , "" }) \
                            .getPluginByDevice(FLAGS_d);
```

##### 2.2 Part II - Read IR Generated by ModelOptimizer (.xml and .bin files)
```cpp
std::string binFileName = fileNameNoExt(FLAGS_m) + ".bin";
CNNNetReader networkReader;
networkReader.ReadNetwork(FLAGS_m);  // Reading network model
networkReader.ReadWeights(binFileName);  // Loading weights
CNNNetwork network = networkReader.getNetwork();  // Extracting model
```

##### 2.3 Part III - Prepare input blobs
```cpp
InputsDataMap inputInfo = network.getInputsInfo();
auto inputInfoItem = *inputInfo.begin();
inputInfoItem.second->setPrecision(Precision::U8);
inputInfoItem.second->setLayout(Layout::NCHW);

std::vector<std::shared_ptr<unsigned char>> imagesData;
for (auto & i : imageNames) {
    FormatReader::ReaderPtr reader(i.c_str());
    std::shared_ptr<unsigned char> data(
        reader->getData(inputInfoItem.second->getTensorDesc().getDims()[3],
                        inputInfoItem.second->getTensorDesc().getDims()[2]));
    if (data.get() != nullptr) {
        imagesData.push_back(data);
    }
}

network.setBatchSize(imagesData.size());  // Setting batch size using image count
```

##### 2.4 Part IV - Prepare output blobs
```cpp
OutputsDataMap outputInfo(network.getOutputsInfo());

std::string firstOutputName;
for (auto & item : outputInfo) {
    if (firstOutputName.empty()) {
        firstOutputName = item.first;
    }
    DataPtr outputData = item.second;
    item.second->setPrecision(Precision::FP32);
}

const SizeVector outputDims = outputInfo.begin()->second->getDims();
```

##### 2.5 Part V - Loading model to the plugin
```cpp
ExecutableNetwork executable_network = plugin.LoadNetwork(network, {});
inputInfoItem.second = {};
outputInfo = {};
network = {};
networkReader = {};
```

##### 2.6 Part VI - Create infer request
```cpp
InferRequest infer_request = executable_network.CreateInferRequest();
```

##### 2.7 Part VII - Prepare input
```cpp
for (const auto & item : inputInfo) {
    Blob::Ptr input = infer_request.GetBlob(item.first);
    
    size_t num_channels = input->getTensorDesc().getDims()[1];
    size_t image_size = input->getTensorDesc().getDims()[2] * input->getTensorDesc().getDims()[3];

    auto data = input->buffer().as<PrecisionTrait<Precision::U8>::value_type*>();
    
    for (size_t image_id = 0; image_id < imagesData.size(); ++image_id) {
        for (size_t pid = 0; pid < image_size; pid++) {
            for (size_t ch = 0; ch < num_channels; ++ch) {
                //[images stride + channels stride + pixel id ]
                data[image_id * image_size * num_channels + ch * image_size + pid ] = imagesData.at(image_id).get()[pid*num_channels + ch];
            }
        }
    }
    
}
inputInfo = {};
```

##### 2.8 Part VIII - Do inference
```cpp
typedef std::chrono::high_resolution_clock Time;
typedef std::chrono::duration<double, std::ratio<1, 1000>> ms;
typedef std::chrono::duration<float> fsec;

double total = 0.0;

for (int iter = 0; iter < FLAGS_ni; ++iter) {
    auto t0 = Time::now();
    infer_request.Infer();
    auto t1 = Time::now();
    fsec fs = t1 - t0;
    ms d = std::chrono::duration_cast<ms>(fs);
    total += d.count();
}
```

##### 2.9 End - Print the result
```
……
```
